"""
COMPETENCIA DE MODELOS DE CLASIFICACI√ìN
========================================

Este script demuestra c√≥mo comparar m√∫ltiples modelos de clasificaci√≥n
para identificar cu√°l se ajusta mejor a un problema espec√≠fico.

Dataset: Breast Cancer Wisconsin (Diagn√≥stico)
Objetivo: Predecir si un tumor es maligno (M) o benigno (B)

MODELOS A COMPARAR:
1. Regresi√≥n Log√≠stica
2. K-Nearest Neighbors (KNN)
3. √Årbol de Decisi√≥n
4. Random Forest
5. Support Vector Machine (SVM)
6. Gaussian Naive Bayes
7. Gradient Boosting
8. AdaBoost

M√âTRICAS DE EVALUACI√ìN:
- Accuracy: Proporci√≥n de predicciones correctas
- Precision: De los predichos como positivos, cu√°ntos lo son realmente
- Recall: De los positivos reales, cu√°ntos fueron detectados
- F1-Score: Media arm√≥nica de precision y recall
- ROC-AUC: √Årea bajo la curva ROC (capacidad discriminativa)

Autor: Script educativo para comparaci√≥n de modelos
Fecha: 2026
"""

# ============================================================
# IMPORTACI√ìN DE LIBRER√çAS
# ============================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, confusion_matrix, classification_report,
    roc_curve
)

# Modelos de Clasificaci√≥n
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# Configuraci√≥n
import warnings
warnings.filterwarnings('ignore')
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 100

print("="*80)
print("COMPETENCIA DE MODELOS DE CLASIFICACI√ìN")
print("Comparaci√≥n de 8 Algoritmos en Dataset de C√°ncer de Mama")
print("="*80 + "\n")

# ============================================================
# 1. CARGA Y EXPLORACI√ìN DE DATOS
# ============================================================
print("\n1. CARGA Y EXPLORACI√ìN DE DATOS")
print("-" * 80)

# Cargar dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = pd.Series(data.target, name='target')

# Informaci√≥n del dataset
print(f"\nüìä INFORMACI√ìN DEL DATASET:")
print(f"  - Total de muestras: {X.shape[0]}")
print(f"  - N√∫mero de caracter√≠sticas: {X.shape[1]}")
print(f"  - Clases: {data.target_names}")
print(f"  - Distribuci√≥n de clases:")
print(f"    ‚Ä¢ Maligno (0): {(y == 0).sum()} ({(y == 0).sum()/len(y)*100:.1f}%)")
print(f"    ‚Ä¢ Benigno (1): {(y == 1).sum()} ({(y == 1).sum()/len(y)*100:.1f}%)")

# Crear DataFrame completo
df = pd.DataFrame(X, columns=data.feature_names)
df['diagnosis'] = y
df['diagnosis_name'] = df['diagnosis'].map({0: 'Maligno', 1: 'Benigno'})

print("\nüìã Primeras caracter√≠sticas del dataset:")
print(df[['mean radius', 'mean texture', 'mean area', 'diagnosis_name']].head())

print("\nüìà Estad√≠sticas b√°sicas de las primeras 5 caracter√≠sticas:")
print(df.iloc[:, :5].describe().round(2))

# ============================================================
# 2. VISUALIZACI√ìN DE DATOS
# ============================================================
print("\n2. VISUALIZACI√ìN DE DATOS")
print("-" * 80)

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Distribuci√≥n de clases
class_counts = df['diagnosis_name'].value_counts()
colors_pie = ['#ff6b6b', '#51cf66']
axes[0, 0].pie(class_counts.values, labels=class_counts.index, autopct='%1.1f%%',
               colors=colors_pie, startangle=90, explode=(0.05, 0))
axes[0, 0].set_title('Distribuci√≥n de Diagn√≥sticos', fontsize=12, fontweight='bold')

# 2. Comparaci√≥n de caracter√≠sticas principales
features_to_plot = ['mean radius', 'mean texture', 'mean area', 'mean smoothness']
df_melted = df[features_to_plot + ['diagnosis_name']].melt(
    id_vars='diagnosis_name', var_name='Caracter√≠stica', value_name='Valor'
)
sns.violinplot(data=df_melted, x='Caracter√≠stica', y='Valor', hue='diagnosis_name',
               split=True, ax=axes[0, 1], palette=['#ff6b6b', '#51cf66'])
axes[0, 1].set_title('Distribuci√≥n de Caracter√≠sticas por Diagn√≥stico', fontsize=12, fontweight='bold')
axes[0, 1].tick_params(axis='x', rotation=15)
axes[0, 1].legend(title='Diagn√≥stico')

# 3. Correlaci√≥n entre caracter√≠sticas principales
correlation_features = ['mean radius', 'mean texture', 'mean perimeter', 
                       'mean area', 'mean smoothness', 'diagnosis']
corr_matrix = df[correlation_features].corr()
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', 
            center=0, ax=axes[1, 0], cbar_kws={'label': 'Correlaci√≥n'})
axes[1, 0].set_title('Matriz de Correlaci√≥n', fontsize=12, fontweight='bold')

# 4. Scatter plot de dos caracter√≠sticas principales
axes[1, 1].scatter(df[df['diagnosis']==1]['mean radius'], 
                   df[df['diagnosis']==1]['mean area'],
                   alpha=0.6, s=50, c='#51cf66', edgecolors='black', 
                   linewidth=0.5, label='Benigno')
axes[1, 1].scatter(df[df['diagnosis']==0]['mean radius'], 
                   df[df['diagnosis']==0]['mean area'],
                   alpha=0.6, s=50, c='#ff6b6b', edgecolors='black', 
                   linewidth=0.5, label='Maligno')
axes[1, 1].set_xlabel('Radio Medio', fontsize=11, fontweight='bold')
axes[1, 1].set_ylabel('√Årea Media', fontsize=11, fontweight='bold')
axes[1, 1].set_title('Separaci√≥n de Clases: Radio vs √Årea', fontsize=12, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('imgs/06_clasificacion_comp_1_exploracion_datos.jpg', format='jpg', bbox_inches='tight', dpi=100)
plt.show()

# ============================================================
# 3. PREPARACI√ìN DE DATOS
# ============================================================
print("\n3. PREPARACI√ìN DE DATOS")
print("-" * 80)

# Divisi√≥n en conjunto de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\n‚úÇÔ∏è Divisi√≥n de datos:")
print(f"  - Entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/X.shape[0]*100:.1f}%)")
print(f"  - Prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/X.shape[0]*100:.1f}%)")

# Normalizaci√≥n de datos
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("\nüîÑ Normalizaci√≥n completada con StandardScaler")
print("   (Media = 0, Desviaci√≥n Est√°ndar = 1)")

# ============================================================
# 4. DEFINICI√ìN DE MODELOS
# ============================================================
print("\n4. DEFINICI√ìN DE MODELOS")
print("-" * 80)

# Diccionario de modelos a comparar
models = {
    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=42),
    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),
    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),
    'Gaussian Naive Bayes': GaussianNB(),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'AdaBoost': AdaBoostClassifier(n_estimators=50, random_state=42)
}

print(f"\nü§ñ Se comparar√°n {len(models)} modelos de clasificaci√≥n:")
for i, name in enumerate(models.keys(), 1):
    print(f"  {i}. {name}")

# ============================================================
# 5. ENTRENAMIENTO Y EVALUACI√ìN DE MODELOS
# ============================================================
print("\n5. ENTRENAMIENTO Y EVALUACI√ìN DE MODELOS")
print("-" * 80)

results = []

for name, model in models.items():
    print(f"\n‚öôÔ∏è Entrenando {name}...", end=" ")
    
    # Entrenar modelo
    model.fit(X_train_scaled, y_train)
    
    # Predicciones
    y_pred = model.predict(X_test_scaled)
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else y_pred
    
    # Calcular m√©tricas
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_pred_proba)
    
    # Validaci√≥n cruzada
    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    # Guardar resultados
    results.append({
        'Modelo': name,
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1-Score': f1,
        'ROC-AUC': roc_auc,
        'CV Accuracy': cv_mean,
        'CV Std': cv_std
    })
    
    print(f"‚úì Accuracy: {accuracy:.4f} | F1: {f1:.4f}")

# Crear DataFrame con resultados
df_results = pd.DataFrame(results)
df_results = df_results.sort_values('F1-Score', ascending=False).reset_index(drop=True)

print("\n" + "="*80)
print("RESULTADOS DE LA COMPETENCIA")
print("="*80)
print(df_results.to_string(index=False))
print("="*80 + "\n")

# ============================================================
# 6. VISUALIZACI√ìN DE COMPARACI√ìN DE MODELOS
# ============================================================
print("\n6. VISUALIZACI√ìN DE COMPARACI√ìN DE MODELOS")
print("-" * 80)

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Comparaci√≥n de Accuracy
colors = sns.color_palette("viridis", len(df_results))
axes[0, 0].barh(df_results['Modelo'], df_results['Accuracy'], color=colors, edgecolor='black')
axes[0, 0].set_xlabel('Accuracy', fontsize=11, fontweight='bold')
axes[0, 0].set_title('Comparaci√≥n de Accuracy por Modelo', fontsize=12, fontweight='bold')
axes[0, 0].set_xlim(0.85, 1.0)
axes[0, 0].grid(axis='x', alpha=0.3)
for i, v in enumerate(df_results['Accuracy']):
    axes[0, 0].text(v + 0.002, i, f'{v:.4f}', va='center', fontsize=9, fontweight='bold')

# 2. Comparaci√≥n de F1-Score
axes[0, 1].barh(df_results['Modelo'], df_results['F1-Score'], color=colors, edgecolor='black')
axes[0, 1].set_xlabel('F1-Score', fontsize=11, fontweight='bold')
axes[0, 1].set_title('Comparaci√≥n de F1-Score por Modelo', fontsize=12, fontweight='bold')
axes[0, 1].set_xlim(0.85, 1.0)
axes[0, 1].grid(axis='x', alpha=0.3)
for i, v in enumerate(df_results['F1-Score']):
    axes[0, 1].text(v + 0.002, i, f'{v:.4f}', va='center', fontsize=9, fontweight='bold')

# 3. Precision vs Recall
axes[1, 0].scatter(df_results['Precision'], df_results['Recall'], 
                  s=200, alpha=0.6, c=range(len(df_results)), 
                  cmap='viridis', edgecolors='black', linewidth=2)
for i, model in enumerate(df_results['Modelo']):
    axes[1, 0].annotate(model, 
                       (df_results.iloc[i]['Precision'], df_results.iloc[i]['Recall']),
                       fontsize=8, ha='right', va='bottom')
axes[1, 0].set_xlabel('Precision', fontsize=11, fontweight='bold')
axes[1, 0].set_ylabel('Recall', fontsize=11, fontweight='bold')
axes[1, 0].set_title('Precision vs Recall', fontsize=12, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)
axes[1, 0].set_xlim(0.85, 1.0)
axes[1, 0].set_ylim(0.85, 1.0)

# 4. Comparaci√≥n de m√∫ltiples m√©tricas (Radar)
# Seleccionar top 5 modelos
top_5 = df_results.head(5)
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
angles += angles[:1]

axes[1, 1] = plt.subplot(2, 2, 4, projection='polar')
for i, row in top_5.iterrows():
    values = [row[m] for m in metrics]
    values += values[:1]
    axes[1, 1].plot(angles, values, 'o-', linewidth=2, label=row['Modelo'])
    axes[1, 1].fill(angles, values, alpha=0.15)

axes[1, 1].set_xticks(angles[:-1])
axes[1, 1].set_xticklabels(metrics, fontsize=9)
axes[1, 1].set_ylim(0.85, 1.0)
axes[1, 1].set_title('Comparaci√≥n Multim√©trica (Top 5)', fontsize=12, fontweight='bold', pad=20)
axes[1, 1].legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=8)
axes[1, 1].grid(True)

plt.tight_layout()
plt.savefig('imgs/06_clasificacion_comp_2_comparacion_metricas.jpg', format='jpg', bbox_inches='tight', dpi=100)
plt.show()

# ============================================================
# 7. AN√ÅLISIS DEL MEJOR MODELO
# ============================================================
print("\n7. AN√ÅLISIS DEL MEJOR MODELO")
print("-" * 80)

# Seleccionar mejor modelo
best_model_name = df_results.iloc[0]['Modelo']
best_model = models[best_model_name]

print(f"\nüèÜ MEJOR MODELO: {best_model_name}")
print(f"\nM√©tricas en conjunto de prueba:")
print(f"  - Accuracy:  {df_results.iloc[0]['Accuracy']:.4f}")
print(f"  - Precision: {df_results.iloc[0]['Precision']:.4f}")
print(f"  - Recall:    {df_results.iloc[0]['Recall']:.4f}")
print(f"  - F1-Score:  {df_results.iloc[0]['F1-Score']:.4f}")
print(f"  - ROC-AUC:   {df_results.iloc[0]['ROC-AUC']:.4f}")
print(f"\nValidaci√≥n Cruzada (5-fold):")
print(f"  - CV Accuracy: {df_results.iloc[0]['CV Accuracy']:.4f} ¬± {df_results.iloc[0]['CV Std']:.4f}")

# Predicciones del mejor modelo
y_pred_best = best_model.predict(X_test_scaled)
y_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]

# Reporte de clasificaci√≥n
print("\nüìä REPORTE DE CLASIFICACI√ìN DETALLADO:")
print(classification_report(y_test, y_pred_best, target_names=data.target_names))

# ============================================================
# 8. MATRIZ DE CONFUSI√ìN Y CURVA ROC
# ============================================================
print("\n8. MATRIZ DE CONFUSI√ìN Y CURVA ROC")
print("-" * 80)

fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred_best)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],
            xticklabels=data.target_names, yticklabels=data.target_names,
            cbar_kws={'label': 'N√∫mero de casos'})
axes[0].set_xlabel('Predicci√≥n', fontsize=11, fontweight='bold')
axes[0].set_ylabel('Valor Real', fontsize=11, fontweight='bold')
axes[0].set_title(f'Matriz de Confusi√≥n - {best_model_name}', fontsize=12, fontweight='bold')

# A√±adir porcentajes
for i in range(2):
    for j in range(2):
        percentage = cm[i, j] / cm.sum() * 100
        axes[0].text(j + 0.5, i + 0.7, f'({percentage:.1f}%)', 
                    ha='center', va='center', fontsize=9, color='red')

# Curva ROC
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)
roc_auc = df_results.iloc[0]['ROC-AUC']

axes[1].plot(fpr, tpr, color='darkorange', lw=2, 
            label=f'{best_model_name} (AUC = {roc_auc:.4f})')
axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Clasificador Aleatorio')
axes[1].set_xlim([0.0, 1.0])
axes[1].set_ylim([0.0, 1.05])
axes[1].set_xlabel('Tasa de Falsos Positivos (FPR)', fontsize=11, fontweight='bold')
axes[1].set_ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=11, fontweight='bold')
axes[1].set_title('Curva ROC (Receiver Operating Characteristic)', fontsize=12, fontweight='bold')
axes[1].legend(loc="lower right")
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('imgs/06_clasificacion_comp_3_mejor_modelo_confusion_roc.jpg', format='jpg', bbox_inches='tight', dpi=100)
plt.show()

print(f"\n‚úì Matriz de confusi√≥n generada")
print(f"  - Verdaderos Negativos: {cm[0,0]}")
print(f"  - Falsos Positivos: {cm[0,1]}")
print(f"  - Falsos Negativos: {cm[1,0]}")
print(f"  - Verdaderos Positivos: {cm[1,1]}")

# ============================================================
# 9. CURVAS ROC DE TODOS LOS MODELOS
# ============================================================
print("\n9. CURVAS ROC DE TODOS LOS MODELOS")
print("-" * 80)

plt.figure(figsize=(10, 8))

for name, model in models.items():
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, 'predict_proba') else model.predict(X_test_scaled)
    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
    auc = roc_auc_score(y_test, y_pred_proba)
    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {auc:.3f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Clasificador Aleatorio (AUC = 0.500)')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Tasa de Falsos Positivos (FPR)', fontsize=12, fontweight='bold')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=12, fontweight='bold')
plt.title('Curvas ROC - Comparaci√≥n de Todos los Modelos', fontsize=13, fontweight='bold')
plt.legend(loc="lower right", fontsize=9)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('imgs/06_clasificacion_comp_4_curvas_roc_todos.jpg', format='jpg', bbox_inches='tight', dpi=100)
plt.show()

# ============================================================
# 10. AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS
# ============================================================
print("\n10. AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS")
print("-" * 80)

# Verificar si el mejor modelo tiene feature importance
if hasattr(best_model, 'feature_importances_'):
    importances = best_model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    print(f"\nüîç Top 10 caracter√≠sticas m√°s importantes para {best_model_name}:")
    for i in range(min(10, len(indices))):
        print(f"  {i+1}. {data.feature_names[indices[i]]:<30} - {importances[indices[i]]:.4f}")
    
    # Visualizaci√≥n
    plt.figure(figsize=(10, 6))
    top_n = 15
    plt.barh(range(top_n), importances[indices[:top_n]][::-1], 
            color='steelblue', edgecolor='black')
    plt.yticks(range(top_n), [data.feature_names[i] for i in indices[:top_n]][::-1])
    plt.xlabel('Importancia', fontsize=12, fontweight='bold')
    plt.title(f'Top {top_n} Caracter√≠sticas m√°s Importantes - {best_model_name}', 
             fontsize=13, fontweight='bold')
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.savefig('imgs/06_clasificacion_comp_5_feature_importance.jpg', format='jpg', bbox_inches='tight', dpi=100)
    plt.show()
    
elif hasattr(best_model, 'coef_'):
    coefficients = np.abs(best_model.coef_[0])
    indices = np.argsort(coefficients)[::-1]
    
    print(f"\nüîç Top 10 caracter√≠sticas con mayor peso para {best_model_name}:")
    for i in range(min(10, len(indices))):
        print(f"  {i+1}. {data.feature_names[indices[i]]:<30} - {coefficients[indices[i]]:.4f}")
    
    # Visualizaci√≥n
    plt.figure(figsize=(10, 6))
    top_n = 15
    plt.barh(range(top_n), coefficients[indices[:top_n]][::-1], 
            color='coral', edgecolor='black')
    plt.yticks(range(top_n), [data.feature_names[i] for i in indices[:top_n]][::-1])
    plt.xlabel('Peso Absoluto del Coeficiente', fontsize=12, fontweight='bold')
    plt.title(f'Top {top_n} Caracter√≠sticas con Mayor Peso - {best_model_name}', 
             fontsize=13, fontweight='bold')
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.savefig('imgs/06_clasificacion_comp_5_feature_importance.jpg', format='jpg', bbox_inches='tight', dpi=100)
    plt.show()
else:
    print(f"\n‚ö†Ô∏è {best_model_name} no proporciona importancia de caracter√≠sticas directamente")

# ============================================================
# RESUMEN FINAL
# ============================================================
print("\n" + "="*80)
print("RESUMEN FINAL")
print("="*80)
print(f"""
Dataset: Breast Cancer Wisconsin
Total de muestras: {X.shape[0]}
Caracter√≠sticas: {X.shape[1]}
Clases: Maligno (0) / Benigno (1)

PROCESO REALIZADO:
1. ‚úì Exploraci√≥n de datos con estad√≠sticas y visualizaciones
2. ‚úì An√°lisis de distribuci√≥n de clases
3. ‚úì Normalizaci√≥n de datos con StandardScaler
4. ‚úì Comparaci√≥n de {len(models)} modelos de clasificaci√≥n
5. ‚úì Evaluaci√≥n con validaci√≥n cruzada (5 folds)
6. ‚úì Selecci√≥n del mejor modelo: {best_model_name}
7. ‚úì An√°lisis de matriz de confusi√≥n y curva ROC
8. ‚úì An√°lisis de importancia de caracter√≠sticas

COMPARACI√ìN DE MODELOS:
Los {len(models)} modelos fueron evaluados usando m√∫ltiples m√©tricas.
Top 3 modelos por F1-Score:
""")

for i in range(min(3, len(df_results))):
    modelo = df_results.iloc[i]
    print(f"  {i+1}. {modelo['Modelo']:<25} - F1: {modelo['F1-Score']:.4f}, Accuracy: {modelo['Accuracy']:.4f}")

print(f"""
M√âTRICAS DEL MEJOR MODELO ({best_model_name}):
  - Accuracy:  {df_results.iloc[0]['Accuracy']:.4f} - Proporci√≥n de predicciones correctas
  - Precision: {df_results.iloc[0]['Precision']:.4f} - De los predichos positivos, cu√°ntos son correctos
  - Recall:    {df_results.iloc[0]['Recall']:.4f} - De los positivos reales, cu√°ntos fueron detectados
  - F1-Score:  {df_results.iloc[0]['F1-Score']:.4f} - Media arm√≥nica de precision y recall
  - ROC-AUC:   {df_results.iloc[0]['ROC-AUC']:.4f} - Capacidad discriminativa del modelo

INTERPRETACI√ìN:
‚úì El modelo {best_model_name} demostr√≥ el mejor rendimiento general
‚úì Accuracy de {df_results.iloc[0]['Accuracy']:.4f} indica {df_results.iloc[0]['Accuracy']*100:.1f}% de predicciones correctas
‚úì F1-Score de {df_results.iloc[0]['F1-Score']:.4f} muestra excelente balance precision-recall
‚úì ROC-AUC de {df_results.iloc[0]['ROC-AUC']:.4f} indica excelente capacidad discriminativa

VALIDACI√ìN CRUZADA:
El modelo fue validado con 5-fold cross-validation para asegurar
que no hay sobreajuste y que generaliza bien a datos nuevos.
CV Accuracy: {df_results.iloc[0]['CV Accuracy']:.4f} ¬± {df_results.iloc[0]['CV Std']:.4f}
""")
print("="*80 + "\n")

print("üí° RECOMENDACIONES PARA MEJORAR:")
print("  1. Feature Engineering: Crear interacciones entre caracter√≠sticas")
print("  2. Tuning de Hiperpar√°metros: Usar GridSearchCV o RandomizedSearchCV")
print("  3. Ensemble Methods: Combinar modelos con Voting o Stacking")
print("  4. An√°lisis de Errores: Estudiar casos mal clasificados")
print("  5. Balanceo de Clases: Si hay desbalance, usar SMOTE o class_weight")
print("\n" + "="*80 + "\n")

print("üìä M√âTRICAS DE CLASIFICACI√ìN EXPLICADAS:")
print("  ‚Ä¢ Accuracy:  % total de aciertos (puede ser enga√±osa con clases desbalanceadas)")
print("  ‚Ä¢ Precision: De los que predije como positivos, cu√°ntos realmente lo son")
print("  ‚Ä¢ Recall:    De todos los positivos reales, cu√°ntos logr√© detectar")
print("  ‚Ä¢ F1-Score:  Balance entre precision y recall (ideal cuando importan ambos)")
print("  ‚Ä¢ ROC-AUC:   Qu√© tan bien separa el modelo las clases (1.0 = perfecto)")
print("\n" + "="*80 + "\n")

print("üéØ CU√ÅNDO USAR CADA MODELO:")
print("  ‚Ä¢ Logistic Regression: R√°pido, interpretable, bueno para baseline")
print("  ‚Ä¢ Random Forest: Robusto, maneja no-linealidad, menos overfitting")
print("  ‚Ä¢ SVM: Excelente para datasets peque√±os/medianos con buena separaci√≥n")
print("  ‚Ä¢ Gradient Boosting: Alto rendimiento, pero m√°s lento de entrenar")
print("  ‚Ä¢ KNN: Simple, no requiere entrenamiento, pero lento en predicci√≥n")
print("  ‚Ä¢ Naive Bayes: Muy r√°pido, asume independencia entre caracter√≠sticas")
print("\n" + "="*80 + "\n")
